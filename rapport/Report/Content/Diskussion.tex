\section{Diskussion}
For at først at kigge på mulighederne for at kode til en \textit{GPU} til C \#, som er det sprog \textit{Corecalc} er skrevet i, er der blevet lavet nogen forskellige test der giver nogen interessante resultater. Vis man kigger på \textit{C++ AMP} og \textit{CUDA} i tabellen \ref{fig:samletC++} kan det ses at C++ AMP virker til at være meget bedre en CUDA, dette tilfælde kunne skylde at \textit{C++ AMP} måske har en smule optimering af data til tråde, fordi i \textit{C++ AMP} skal der ikke findes ud af hvor mange tråde og blokke der skal bruges til at køre en funktion, men i sted giver man et array man vil havde gået igennem på GPU. hvorimod i \textit{CUDA} skal man selv rode med hvor mange tråde og blokke der skal bruges.

En overraskende ting kan ses i CUDAfy 1d og 2d kan det ses, at det er hurtigere at, hvis håndterer data i 1d array i sted for 2d array, når matrixen begunder at blive have den størrelse være 10 eller over. Hvilket er det modsatte af hvad artiklen \textit{Adaptive Input-aware Compilation for Graphics Engines}\cite{samadi2012adaptive} fokus er.

Det største problem jeg har haft med med CUDA og CUDAfy er, at man selv skal finde ud af hvor mange blokke og antal tråde pr. blok. Hvilket godt kan give nogen problemer når man arbejder med et program der skal arbejde med varierende input. Efter hvad jeg ar fundet på nettet angående problem med hvor mange blokke og antal tråde pr. blok man skal bruge, har jeg for det meste fundet at man skal slev teste sig frem alt efter hvad der virker godt på det hardware man tester på. Noget man også kunne kigge på er om artiklen\textit{Adaptive Input-aware Compilation for Graphics Engines}\cite{samadi2012adaptive} Kunne bruges til at øge hvor effektiv GPU funktion er. For at løse problemet med varierede tråde mænge, har jeg fremstillet en generist metode der finder ud af hvor mange blokke der skal bruges, hvis det hele ikke kan gøres på en blok, denne metode er dog ikke perfekt. Siden der kunne laves noget optimering af hvordan data bliver gemt til GPU

En anden ting jeg er kommet på er plads mangle på GPU'en, min GPU kunne kun klare at gange matrixer der har max størrelse på 1023. En løsning på dette problem kunne være at begynde at bruge billede hukommelse på GPU'en til readonly data. Hvilke skulle være muligt med CUDA, men desværre ikke skulle være muligt med CUDAfy på dette tidspunkt.

Igennem resulterende af testen\ref{result_GPU_Funcalc} for GPU funktion og den simple måde at lave funktioner til \textit{Funcalc}, er det første ting der viser sig at jo mere data der bliver arbejdet med jo hurtigere bliver det bedre at udregne på GPU, dette tilfælde kunne tilfælde kunne skylde måden data bliver gemt i \textit{Funcalc}, hvor der kan være langt imellem den data der skal bruges, hvorimod på GPU vil data ligge ved siden af hinanden for den enkelte tråd, så derved bliver der ikke brugt nær så meget tid på dette. Noget andet der kan ses ud fra testen, er af GPU er stort set en vandret linje, hvorimod at den normal at stille den form for funktion op, bliver til en linje der cirka går med en 45\% stigning.
Derved kan man sige at hvis en funktion er stor nok eller der er mange data der skal arbejdes, kunne GPU funktion godt bruges